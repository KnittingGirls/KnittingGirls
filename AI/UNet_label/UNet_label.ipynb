{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJQVImm0qtoE"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jarvis-Geun/DeepLearning-Wiki/blob/main/Semantic-Segmentation/U-Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "napowr-HqtoH"
      },
      "source": [
        "### Reference\n",
        "- [Semantic Segmentation in Self-driving Cars](https://blog.jovian.ai/semantic-segmentation-in-self-driving-cars-3cb89aa08e9b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcIzxxAlqtoI"
      },
      "source": [
        "## Step 1. 모델 설계"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrttAvKnqtoI"
      },
      "source": [
        "### 1.1 라이브러리 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QicssGw_qtoJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJmTSWVHqtoK"
      },
      "source": [
        "모델 설계하는데 필요한 라이브러리를 불러옵니다.\n",
        "[PIL 라이브러리](https://ko.wikipedia.org/wiki/Python_Imaging_Library)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4G5y8VhqtoK"
      },
      "source": [
        "### 1.2 GPU 설정하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTLePqAKqtoK",
        "outputId": "2097f2b4-626b-485b-c08f-94af62ab6eac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# GPU 사용이 가능할 경우, GPU를 사용할 수 있게 함.\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jOTpdxjqtoL"
      },
      "source": [
        "### 1.3 파일 시스템"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXhs_5eGtex2",
        "outputId": "8542c8bc-f5f0-4466-bcc0-587e2b2bcf85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# root_path = '/content/drive/MyDrive/start-knitting-girls/UNet/dataset/dataset/' # 입력 이미지 폴더, /content/ 추가\n",
        "\n",
        "# data_dir = root_path\n",
        "\n",
        "# # data_dir의 경로(문자열)와 train(문자열)을 결합해서 train_dir(train 폴더의 경로)에 저장합니다.\n",
        "# train_dir = os.path.join(data_dir, \"train/images\")\n",
        "\n",
        "# # data_dir의 경로(문자열)와 val(문자열)을 결합해서 val_dir(val 폴더의 경로)에 저장합니다.\n",
        "# val_dir = os.path.join(data_dir, \"val/images\")\n",
        "\n",
        "# # train_dir 경로에 있는 모든 파일을 리스트의 형태로 불러와서 train_fns에 저장합니다.\n",
        "# train_fns = os.listdir(train_dir)\n",
        "\n",
        "# # val_dir 경로에 있는 모든 파일을 리스트의 형태로 불러와서 val_fns에 저장합니다.\n",
        "# val_fns = os.listdir(val_dir)"
      ],
      "metadata": {
        "id": "AjNvieoGttqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각각의 데이터 경로 설정\n",
        "# data_dir = ''\n",
        "train_dir = ''\n",
        "val_dir = ''\n",
        "\n",
        "train_fns = os.listdir(train_dir)\n",
        "\n",
        "val_fns = os.listdir(val_dir)"
      ],
      "metadata": {
        "id": "maysS3wdlSLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# root_path = '/content/drive/MyDrive/start-knitting-girls/UNet/dataset/sweater/' # 입력 이미지 폴더, /content/ 추가\n",
        "\n",
        "# data_dir = root_path\n",
        "\n",
        "# # data_dir의 경로(문자열)와 train(문자열)을 결합해서 train_dir(train 폴더의 경로)에 저장합니다.\n",
        "# train_dir = os.path.join(data_dir, \"train\")\n",
        "\n",
        "# # data_dir의 경로(문자열)와 val(문자열)을 결합해서 val_dir(val 폴더의 경로)에 저장합니다.\n",
        "# val_dir = os.path.join(data_dir, \"val\")\n",
        "\n",
        "# # train_dir 경로에 있는 모든 파일을 리스트의 형태로 불러와서 train_fns에 저장합니다.\n",
        "# train_fns = os.listdir(train_dir)\n",
        "\n",
        "# # val_dir 경로에 있는 모든 파일을 리스트의 형태로 불러와서 val_fns에 저장합니다.\n",
        "# val_fns = os.listdir(val_dir)"
      ],
      "metadata": {
        "id": "EcYCTa-670tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHVGN8pIqtoM"
      },
      "source": [
        "train_fns의 길이는 2975이며 val_fns의 길이는 500입니다. 이는 데이터셋인 **Cityscape Dataset**의 학습(train) 및 검증(validation) 데이터와 일치하는 것을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYYkM3kTqtoM"
      },
      "source": [
        "### 1.4 샘플 이미지 검색\n",
        "경로를 지정했으므로 이제 이 경로를 사용하여 샘플 이미지를 불러오도록 하겠습니다. 이 과정은 생략해도 되지만 불러오는 과정이 원활하게 동작하는지 확인하기 위해 실습해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcoYB53LqtoN"
      },
      "outputs": [],
      "source": [
        "# train_dir(문자열)와 train_fns[0](문자열)의 경로를 결합하여 sample_image_fp(샘플 이미지의 경로)에 저장합니다.\n",
        "sample_image_fp = os.path.join(train_dir, train_fns[0])\n",
        "\n",
        "# PIL 라이브러리의 Image 모듈을 사용하여, sample_image_fp를 불러옵니다.\n",
        "# RGB 형태로 변환하여 sample_image에 저장하는 것으로 이해했는데, \".convert(\"RGB\")\" 코드는 없어도 될 것 같습니다.\n",
        "# Image.open() 함수 자체가 RGB의 형태로 불러오는 것으로 이해했습니다. 확실하지 않습니다...\n",
        "sample_image = Image.open(sample_image_fp).convert(\"RGB\")\n",
        "\n",
        "plt.imshow(sample_image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG7zRkRwqtoN"
      },
      "source": [
        "[plt.show() vs plt.imshow()](https://stackoverflow.com/questions/3497578/matplotlib-plot-and-imshow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X05qAZQWqtoN"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYo7ROdFqtoN"
      },
      "source": [
        "### 1.5 Output Label 정의하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI-iEaP3qtoN"
      },
      "outputs": [],
      "source": [
        "num_items = 1000\n",
        "\n",
        "# 0~255 사이의 숫자를 3*num_items번 랜덤하게 뽑기\n",
        "color_array = np.random.choice(range(256), 3*num_items).reshape(-1, 3)\n",
        "print(color_array.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv4O6ejKqtoO"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "\n",
        "# K-means clustering 알고리즘을 사용하여 label_model에 저장합니다.\n",
        "label_model = KMeans(n_clusters = num_classes)\n",
        "label_model.fit(color_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFmAt6erqtoO"
      },
      "source": [
        "[K-means clustering](https://blog.mathpresso.com/mathpresso-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%8A%A4%ED%84%B0%EB%94%94-9-%EA%B5%B0%EC%A7%91%ED%99%94-clustering-542390bb4b74)\n",
        "\n",
        "[label_model.fit(color_array)](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#:~:text=fit(X%2C%20y%3DNone%2C%20sample_weight%3DNone))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB9uw_zgqtoO"
      },
      "outputs": [],
      "source": [
        "#  # 이전에 샘플이미지에서 볼 수 있듯이, original image와 labeled image가 연결되어 있는데 이를 분리해줍니다.\n",
        "#  def split_image(image) :\n",
        "#     image = np.array(image)\n",
        "\n",
        "#     # 이미지의 크기가 256 x 512 였는데 이를 original image와 labeled image로 분리하기 위해 리스트로 슬라이싱 합니다.\n",
        "#     # 그리고 분리된 이미지를 각각 cityscape(= original image)와 label(= labeled image)에 저장합니다.\n",
        "#     cityscape, label = image[:, :256, :], image[:, 256:, :]\n",
        "#     return cityscape, label\n",
        "\n",
        "\n",
        "# # # 바로 이전 코드에서 정의한 split_image() 함수를 이용하여 sample_image를 분리한 후, cityscape과 label에 각각 저장합니다.\n",
        "# cityscape, label = split_image(sample_image)\n",
        "\n",
        "# label_class = label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\n",
        "# fig, axes = plt.subplots(1, 3, figsize = (15, 5))\n",
        "# axes[0].imshow(cityscape)\n",
        "# axes[1].imshow(label)\n",
        "# axes[2].imshow(label_class)\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_image(image):\n",
        "    # 이미지 크기 확인\n",
        "    image = np.array(image)\n",
        "    height, width, _ = image.shape\n",
        "    cityscape_width = width // 2  # 이미지의 절반 너비를 계산\n",
        "    cityscape, label = image[:, :cityscape_width, :], image[:, cityscape_width:, :]\n",
        "    return cityscape, label\n",
        "\n",
        "# 이미지 분리\n",
        "cityscape, label = split_image(sample_image)\n",
        "\n",
        "# 레이블 클래스 예측\n",
        "label_height, label_width = label.shape[:2]\n",
        "label_class = label_model.predict(label.reshape(-1, 3)).reshape(label_height, label_width)\n",
        "\n",
        "# 시각화\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "axes[0].imshow(cityscape)\n",
        "axes[1].imshow(label)\n",
        "axes[2].imshow(label_class, cmap='viridis')  # 컬러맵 지정\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "L3EwtnsCvJZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxgHx_nRqtoO"
      },
      "source": [
        "### 1.6 데이터셋 정의하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sod_x9gfqtoO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class CityscapeDataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_dir, label_model, target_size=(256, 256)):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_fns = os.listdir(image_dir)\n",
        "        self.label_model = label_model\n",
        "        self.target_size = target_size  # 원하는 이미지 크기 (가로, 세로)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_fns)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_fn = self.image_fns[index]\n",
        "        image_fp = os.path.join(self.image_dir, image_fn)\n",
        "\n",
        "        # 이미지 로드\n",
        "        image = Image.open(image_fp).convert(\"RGB\")\n",
        "        image = np.array(image)\n",
        "\n",
        "        # 이미지 분리\n",
        "        cityscape, label = self.split_image(image)\n",
        "\n",
        "        # 크기 조정\n",
        "        cityscape = self.resize_image(cityscape)\n",
        "        label = self.resize_image(label)\n",
        "\n",
        "        # 레이블 클래스 변환\n",
        "        label_class = self.label_model.predict(label.reshape(-1, 3)).reshape(self.target_size[1], self.target_size[0])\n",
        "        label_class = torch.Tensor(label_class).long()\n",
        "\n",
        "        # 시티스케이프 이미지 변환\n",
        "        cityscape = self.transform(cityscape)\n",
        "\n",
        "        return cityscape, label_class\n",
        "\n",
        "    def split_image(self, image):\n",
        "        \"\"\"\n",
        "        이미지를 두 부분으로 나눔: 시티스케이프(좌측)와 레이블(우측).\n",
        "        \"\"\"\n",
        "        cityscape = image[:, :256, :]  # 좌측 256 픽셀\n",
        "        label = image[:, 256:, :]      # 우측 나머지\n",
        "        return cityscape, label\n",
        "\n",
        "    def resize_image(self, image):\n",
        "        \"\"\"\n",
        "        이미지를 target_size로 리사이즈.\n",
        "        \"\"\"\n",
        "        image = Image.fromarray(image)\n",
        "        image = image.resize(self.target_size, Image.BILINEAR)  # Bilinear로 리사이즈\n",
        "        return np.array(image)\n",
        "\n",
        "    def transform(self, image):\n",
        "        \"\"\"\n",
        "        이미지를 PyTorch 텐서로 변환 및 정규화.\n",
        "        \"\"\"\n",
        "        transform_ops = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.485, 0.56, 0.406), std=(0.229, 0.224, 0.225))\n",
        "        ])\n",
        "        return transform_ops(image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P9s_e0dqtoP"
      },
      "outputs": [],
      "source": [
        "# dataset = CityscapeDataset(train_dir, label_model)\n",
        "# print(len(dataset))\n",
        "\n",
        "# cityscape, label_class = dataset[0]\n",
        "# print(cityscape.shape)\n",
        "# print(label_class.shape)\n",
        "\n",
        "# 몇개의 데이터셋, 라벨 클래스의 모양 출력"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Value error\n",
        "\n",
        "# class CityscapeDataset(Dataset):\n",
        "#     def __init__(self, image_dir, label_model):\n",
        "#         self.image_dir = image_dir\n",
        "#         self.image_fns = os.listdir(image_dir)\n",
        "#         self.label_model = label_model\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.image_fns)\n",
        "\n",
        "#     def __getitem__(self, index): # value error 수정\n",
        "#         image_fn = self.image_fns[index]\n",
        "#         image_fp = os.path.join(self.image_dir, image_fn)\n",
        "#         image = Image.open(image_fp)\n",
        "#         image = np.array(image)\n",
        "#         cityscape, label = self.split_image(image)\n",
        "\n",
        "#         # Get the actual height and width of the label image\n",
        "#         label_height = label.shape[0]\n",
        "#         label_width = label.shape[1]\n",
        "\n",
        "#         # Reshape based on actual dimensions\n",
        "#         label_class = self.label_model.predict(label.reshape(-1, 3)).reshape(label_height, label_width)\n",
        "#         label_class = torch.Tensor(label_class).long()\n",
        "#         cityscape = self.transform(cityscape)\n",
        "#         return cityscape, label_class\n",
        "\n",
        "\n",
        "#     def split_image(self, image):\n",
        "#         image = np.array(image)\n",
        "#         # cityscape, label = image[:, :256, :], image[:, 256:, :]\n",
        "#         split_point = image.shape[1] // 2 # 영향을 주지 않음\n",
        "#         cityscape, label = image[:, :split_point, :], image[:, split_point:, :]\n",
        "#         return cityscape, label\n",
        "\n",
        "#     def transform(self, image):\n",
        "#         transform_ops = transforms.Compose([\n",
        "#             transforms.ToTensor(),\n",
        "#             transforms.Normalize(mean=(0.485, 0.56, 0.406), std=(0.229, 0.224, 0.225))\n",
        "#         ])\n",
        "#         return transform_ops(image)\n",
        "\n",
        "# print(cityscape.shape, label_class.shape)\n",
        "# print(len(cityscape), len(label_class))"
      ],
      "metadata": {
        "id": "xGHlA7XAv8sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th0tMNREqtoP"
      },
      "source": [
        "위의 출력 결과를 통해, 학습 데이터가 2975개 있다는 것을 다시 한번 확인할 수 있습니다. 또한, cityscape과 label_class의 shape도 알 수 있습니다. cityscape의 경우 **transforms.ToTensor()** 를 통과하여 [3, 256, 256]의 텐서 형태를 가지게 되는 것을 확인할 수 있습니다.\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj1JL_naqtoP"
      },
      "source": [
        "### 1.7 U-Net 모델 정의하기\n",
        "[이전 포스팅](https://velog.io/@jarvis_geun/U-Net-%ED%86%BA%EC%95%84%EB%B3%B4%EA%B8%B0in-depth)에서 다룬 U-Net 모델을 사용하여 **Sementic Segmentation**을 진행하겠습니다. 아래의 사진을 바탕으로 U-Net 모델을 만듭니다.  \n",
        "  \n",
        "[![](https://images.velog.io/images/jarvis_geun/post/3e74c2f1-a1e9-4248-b9fc-680fce3a0d8c/image.png)](https://velog.io/@jarvis_geun/U-Net-%ED%86%BA%EC%95%84%EB%B3%B4%EA%B8%B0in-depth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk1Yz00XqtoP"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.contracting_11 = self.conv_block(in_channels=3, out_channels=64)\n",
        "        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.contracting_21 = self.conv_block(in_channels=64, out_channels=128)\n",
        "        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.contracting_31 = self.conv_block(in_channels=128, out_channels=256)\n",
        "        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.contracting_41 = self.conv_block(in_channels=256, out_channels=512)\n",
        "        self.contracting_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n",
        "        self.expansive_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.expansive_12 = self.conv_block(in_channels=1024, out_channels=512)\n",
        "        self.expansive_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.expansive_22 = self.conv_block(in_channels=512, out_channels=256)\n",
        "        self.expansive_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.expansive_32 = self.conv_block(in_channels=256, out_channels=128)\n",
        "        self.expansive_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.expansive_42 = self.conv_block(in_channels=128, out_channels=64)\n",
        "        self.output = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "\t# 1x1 convolution layer 추가\n",
        "        self.output1 = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1, stride=1, padding=1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(num_features=out_channels),\n",
        "                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(num_features=out_channels))\n",
        "        return block\n",
        "\n",
        "    def forward(self, X):\n",
        "        contracting_11_out = self.contracting_11(X) # [-1, 64, 256, 256]\n",
        "        contracting_12_out = self.contracting_12(contracting_11_out) # [-1, 64, 128, 128]\n",
        "        contracting_21_out = self.contracting_21(contracting_12_out) # [-1, 128, 128, 128]\n",
        "        contracting_22_out = self.contracting_22(contracting_21_out) # [-1, 128, 64, 64]\n",
        "        contracting_31_out = self.contracting_31(contracting_22_out) # [-1, 256, 64, 64]\n",
        "        contracting_32_out = self.contracting_32(contracting_31_out) # [-1, 256, 32, 32]\n",
        "        contracting_41_out = self.contracting_41(contracting_32_out) # [-1, 512, 32, 32]\n",
        "        contracting_42_out = self.contracting_42(contracting_41_out) # [-1, 512, 16, 16]\n",
        "        middle_out = self.middle(contracting_42_out) # [-1, 1024, 16, 16]\n",
        "        expansive_11_out = self.expansive_11(middle_out) # [-1, 512, 32, 32]\n",
        "        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, contracting_41_out), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n",
        "        expansive_21_out = self.expansive_21(expansive_12_out) # [-1, 256, 64, 64]\n",
        "        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, contracting_31_out), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n",
        "        expansive_31_out = self.expansive_31(expansive_22_out) # [-1, 128, 128, 128]\n",
        "        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, contracting_21_out), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n",
        "        expansive_41_out = self.expansive_41(expansive_32_out) # [-1, 64, 256, 256]\n",
        "        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out, contracting_11_out), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n",
        "        output_out = self.output(expansive_42_out) # [-1, 64, 256, 256] -> [-1, 64, 256, 256]\n",
        "        output_out1 = self.output(output_out) # [-1, num_classes, 256, 256]\n",
        "\n",
        "        return output_out1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwCyXly_qtoP"
      },
      "outputs": [],
      "source": [
        "# AttributeError: '_SingleProcessDataLoaderIter' object has no attribute 'next'\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "model = UNet(num_classes=num_classes)\n",
        "\n",
        "dataset = CityscapeDataset(train_dir, label_model)\n",
        "\n",
        "data_loader = DataLoader(dataset, batch_size = 4)\n",
        "\n",
        "\n",
        "# X, Y = iter(data_loader).next()\n",
        "X, Y = next(iter(data_loader)) # value error\n",
        "print(X.shape, Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt0tacVNqtoQ"
      },
      "outputs": [],
      "source": [
        "Y_pred = model(X)\n",
        "print(Y_pred.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXQrqO3-qtoQ"
      },
      "source": [
        "## Step 2. 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuT4RIuyqtoQ"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "\n",
        "epochs = 50\n",
        "lr = 0.01\n",
        "\n",
        "dataset = CityscapeDataset(train_dir, label_model)\n",
        "data_loader = DataLoader(dataset, batch_size = batch_size)\n",
        "\n",
        "model = UNet(num_classes = num_classes).to(device)\n",
        "\n",
        "# 손실함수 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Optimizer 정의\n",
        "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "\n",
        "\n",
        "step_losses = []\n",
        "epoch_losses = []\n",
        "\n",
        "for epoch in tqdm(range(epochs)) :\n",
        "  epoch_loss = 0\n",
        "\n",
        "  for X, Y in tqdm(data_loader, total = len(data_loader), leave = False) :\n",
        "    X, Y = X.to(device), Y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    Y_pred = model(X)\n",
        "    loss = criterion(Y_pred, Y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "    step_losses.append(loss.item())\n",
        "  epoch_losses.append(epoch_loss/len(data_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfR4x_XoqtoQ"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axes[0].plot(step_losses)\n",
        "axes[1].plot(epoch_losses)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJSbN4EoqtoQ"
      },
      "source": [
        "## Step 3. 모델 평가하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qdW7etVqtoR"
      },
      "outputs": [],
      "source": [
        "model_name = \"UNet.pth\"\n",
        "torch.save(model.state_dict(), root_path + model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_0L8OLQqtoR"
      },
      "outputs": [],
      "source": [
        "model_path = root_path + model_name\n",
        "model_ = UNet(num_classes = num_classes).to(device)\n",
        "model_.load_state_dict(torch.load(model_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e4SF9AJqtoR"
      },
      "outputs": [],
      "source": [
        "test_batch_size = 8\n",
        "dataset = CityscapeDataset(val_dir, label_model)\n",
        "data_loader = DataLoader(dataset, batch_size = test_batch_size)\n",
        "\n",
        "X,Y = next(iter(data_loader))\n",
        "X,Y = X.to(device), Y.to(device)\n",
        "Y_pred = model_(X)\n",
        "print(Y_pred.shape)\n",
        "Y_pred = torch.argmax(Y_pred, dim=1)\n",
        "print(Y_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wQhTd7oqtoR"
      },
      "outputs": [],
      "source": [
        "inverse_transform = transforms.Compose([\n",
        "    transforms.Normalize((-0.485/0.229, -0.456/0.224, -0.406/0.225), (1/0.229, 1/0.224, 1/0.225))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d-0ZSvqqtoR"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(test_batch_size, 3, figsize=(3*5, test_batch_size*5))\n",
        "\n",
        "iou_scores = []\n",
        "\n",
        "for i in range(test_batch_size):\n",
        "\n",
        "    landscape = inverse_transform(X[i]).permute(1, 2, 0).cpu().detach().numpy()\n",
        "    label_class = Y[i].cpu().detach().numpy()\n",
        "    label_class_predicted = Y_pred[i].cpu().detach().numpy()\n",
        "\n",
        "    # IOU score\n",
        "    intersection = np.logical_and(label_class, label_class_predicted)\n",
        "    union = np.logical_or(label_class, label_class_predicted)\n",
        "    iou_score = np.sum(intersection) / np.sum(union)\n",
        "    iou_scores.append(iou_score)\n",
        "\n",
        "    axes[i, 0].imshow(landscape)\n",
        "    axes[i, 0].set_title(\"Landscape\")\n",
        "    axes[i, 1].imshow(label_class)\n",
        "    axes[i, 1].set_title(\"Label Class\")\n",
        "    axes[i, 2].imshow(label_class_predicted)\n",
        "    axes[i, 2].set_title(\"Label Class - Predicted\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Hc9jIFqtoR"
      },
      "source": [
        "## Step 4. IOU Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mgz7XT5qtoS"
      },
      "outputs": [],
      "source": [
        "print(sum(iou_scores) / len(iou_scores))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p0_upK9Wf9P8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}